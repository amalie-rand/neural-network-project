## My code - balanced

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm


dataframe = pd.read_csv('./neural-network-project/EEG.machinelearing_data_BRMH.csv')

def pre_processing(df):
    ### Data pre-processing:
    ## missing data 
    missing = {}
    for idx, boolean in enumerate(df.isna().any()):
        if boolean == True:
            missing[df.columns[idx]] = sum(df.iloc[:,idx].isna())

    # Replacing nan values
    df[list(missing.keys())] = df[list(missing.keys())].fillna(df[list(missing.keys())].median())  

    # Deleting non-relevant columns
    columns_to_delete = ['main.disorder','eeg.date','no.',"Unnamed: 122"]
    df = df.drop(columns_to_delete, axis=1)

    # Standardize all columns except our encoded categorised columns
    columns_standardize = df.columns.difference(['specific.disorder', 'sex'])
    scaler = StandardScaler()
    df[columns_standardize] = scaler.fit_transform(df[columns_standardize])

    ##  Encoding columns and defining X and y
    encoder = OneHotEncoder(sparse_output=False)
    y = encoder.fit_transform(df[['specific.disorder']])

    # Encoding 'sex' as 0 for M and 1 for F
    df['sex'] = df['sex'].map({'M': 0, 'F': 1})  

    X = df.loc[:, df.columns != "specific.disorder"]
    return X,y

X,y = pre_processing(dataframe)


### Neural network
device = torch.device('cpu')

# Manually set random seed - we have set the seed for reproducibility
torch.manual_seed(1021498)

# Number of splits 
k_folds = 10

# Number of iterations
T = 500

acc_list = []

for train, test in KFold(n_splits=k_folds, shuffle=True, random_state=9276).split(X,y):
    guess_dict = {}

    # Use the nn package to define our model and loss function.
    model = torch.nn.Sequential(
        torch.nn.Linear(1144, 1000),
        torch.nn.ReLU(),
        torch.nn.Linear(1000, 500),
        torch.nn.ReLU(),
        torch.nn.Linear(500, 250),
        torch.nn.ReLU(),
        torch.nn.Linear(250, 12),
    )

    model.to(device)
    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')
    
    learning_rate = 1e-4
    weight_decay = 0.05
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    X_train, y_train = X.iloc[train], y[train]
    X_test, y_test = X.iloc[test], y[test]
  
    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)

    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)
    
    train_loss_list = []

    for t in tqdm(range(T)):
        # Forward pass: compute predicted y by passing x to the model.
        y_pred = model(X_train_tensor)

        # Compute and save loss.
        train_loss = loss_fn(y_pred, y_train_tensor)
        optimizer.zero_grad()

        # Backward pass: compute gradient of the loss with respect to model
        train_loss.backward()
       
        # Calling the step function on an Optimizer makes an update to its
        # parameters
        optimizer.step()    

        train_loss_list.append(train_loss.item())

    # used to plot train loss 
    plt.plot(train_loss_list)
    
    y_test_pred = model(X_test_tensor)
    # transforming prediction to guess
    highest = y_test_pred.argmax(1)
    guess = torch.zeros(y_test_pred.shape).scatter(1, highest.unsqueeze (1), 1.0)
    
    # ATTENTION! these 2 functions are not equal to each other. I don't know which is correct. Please help
    test_loss1 = loss_fn(guess, y_test_tensor)
    test_loss2 = loss_fn(y_test_pred, y_test_tensor)
    print(test_loss1,test_loss2.item())

    # What the model guess in a fold
    for item in guess:
        guess_dict[str(item)] = guess_dict.get(str(item), 0 ) + 1
    print(guess_dict)

    acc = (sum(torch.argmax(guess,dim=1) == torch.argmax(y_test_tensor,dim=1))/len(y_test_tensor))*100
    acc_list.append(acc)

# plots train loss for all folds in one
plt.legend(list(range(1,11)),title="Kfolds")
plt.show()

# plot acc for test set
plt.bar(list(range(1,11)),acc_list)
plt.show()

