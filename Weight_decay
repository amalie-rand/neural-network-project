### test of everything 
### main code

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from sklearn.utils import class_weight
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import balanced_accuracy_score

dataframe = pd.read_csv('./neural-network-project/EEG.machinelearing_data_BRMH.csv')

#Data preprocessing
def pre_processing(df):
    missing = {}
    for idx, boolean in enumerate(df.isna().any()):
        if boolean == True:
            missing[df.columns[idx]] = sum(df.iloc[:,idx].isna())

    #Replacing nan values
    df[list(missing.keys())] = df[list(missing.keys())].fillna(df[list(missing.keys())].median())  

    # Deleting non-relevant columns
    columns_to_delete = ['main.disorder','eeg.date','no.',"Unnamed: 122"]
    df = df.drop(columns_to_delete, axis=1)

    # Standardize all columns except our encoded categorised columns
    columns_standardize = df.columns.difference(['specific.disorder', 'sex'])
    scaler = StandardScaler()
    df[columns_standardize] = scaler.fit_transform(df[columns_standardize])

    # Encoding 'sex' as 0 for M and 1 for F
    df['sex'] = df['sex'].map({'M': 0, 'F': 1})

    #Encoding columns and defining X and y
    encoder = OneHotEncoder(sparse_output=False)
    y = encoder.fit_transform(df[['specific.disorder']])  

    X = df.loc[:, df.columns != "specific.disorder"]
    return X,y


def Neural_network(X, y, learning_rate, weight_decay, iterations, balanced):
    device = torch.device('cpu')

    # Manually set random seed for reproducability
    torch.manual_seed(42)

    # Number of K folds and iterations  
    k_folds = 10
    T = iterations

    # Empty lists for future use for confusion matrix
    all_y_test_labels = []
    all_y_pred_labels = []

    balanced_acc_list = []
    acc_list = []
    
    # Lists to store the mean loss 
    accumulated_train_loss = []
    accumulated_test_loss = []

    for train, test in KFold(n_splits=k_folds, shuffle=True, random_state=69).split(X,y):
        # Divides data set into a test and train set 
        X_train, y_train = X.iloc[train], y[train]
        X_test, y_test = X.iloc[test], y[test]

        # Defining our test and train set as tensors   
        X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
        
        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

        # Our neural network model
        model = torch.nn.Sequential(
            torch.nn.Linear(1144, 1000),
            torch.nn.ReLU(),
            torch.nn.Linear(1000, 500),
            torch.nn.ReLU(),
            torch.nn.Linear(500, 250),
            torch.nn.ReLU(),
            torch.nn.Linear(250, 12), 
            # torch.nn.Softmax(),
        )
        model.to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)


        # Defining loss function if balanced or not
        if balanced:
            # Balancing the L2 regularization 
            y_train_1d = np.argmax(y_train, axis=1)
            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_1d), y=y_train_1d)
            class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)

            loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)

        elif not balanced:
            loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')

       
        train_loss_list = []
        test_loss_list = []
        
        for t in tqdm(range(T)):
            # Forward pass: compute predicted y by passing x to the model.
            y_train_pred = model(X_train_tensor)

            # Compute and save loss.
            train_loss = loss_fn(y_train_pred, y_train_tensor)
            optimizer.zero_grad()

            # Backward pass: compute gradient of the loss with respect to model
            train_loss.backward()

            # Calling the step function on an Optimizer makes an update to its parameters
            optimizer.step()   

            # Adds our training loss values to a list 
            train_loss_list.append(train_loss.item())

            # Compute test loss
            with torch.no_grad():
                y_test_pred = model(X_test_tensor)
                test_loss = loss_fn(y_test_pred, y_test_tensor)
                test_loss_list.append(test_loss.item())
            
            if t < len(accumulated_train_loss):
                accumulated_train_loss[t] += train_loss.item()
                accumulated_test_loss[t] += test_loss.item()
            else:
                accumulated_train_loss.append(train_loss.item())
                accumulated_test_loss.append(test_loss.item())

        # Plots training loss 
        # plt.plot(train_loss_list)
        
        if balanced:
            y_test_1d = np.argmax(y_test, axis=1)
            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_test_1d), y=y_test_1d)
            class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)

        y_test_pred = model(X_test_tensor)

        # Test loss balanced and unbalanced
        test_loss = loss_fn(y_test_pred, y_test_tensor)

        loss_fn_unbalanced = torch.nn.CrossEntropyLoss(reduction="mean")
        test_loss_unbalanced =loss_fn_unbalanced(y_test_pred,y_test_tensor)

        # print(f"The balanced test loss is: {test_loss}")
        # print(f"The unbalanced test loss is: {test_loss_unbalanced}")
        
        # Transforming prediction to guess
        highest = y_test_pred.argmax (1)
        guess = torch.zeros(y_test_pred.shape).scatter(1, highest.unsqueeze (1), 1.0)
        # Prints the model's test guess for each K fold 
        guess_dict = {}
        for item in guess:
            guess_dict[str(item)] = guess_dict.get(str(item), 0 ) + 1
        # print(guess_dict)
        
        # Labels used for balanced accuracy 
        y_pred_labels = torch.argmax(y_test_pred, dim=1).cpu().numpy()
        y_test_labels = torch.argmax(y_test_tensor, dim=1).cpu().numpy()
        all_y_test_labels.extend(y_test_labels)
        all_y_pred_labels.extend(y_pred_labels)

        # Balanced accuracy
        balanced_acc = balanced_accuracy_score(all_y_test_labels, all_y_pred_labels)
        # print("Balanced Accuracy:", 100*balanced_acc)
        balanced_acc_list.append(balanced_acc)

        # Overall accuarcy
        acc = (sum(torch.argmax(guess,dim=1) == torch.argmax(y_test_tensor,dim=1))/len(y_test_tensor))
        acc_list.append(acc)

        # Plots weight decay against test loss mean
        # loss_list_plot.append(test_loss.item())
    
    # mean_loss = np.mean(loss_list_plot)
    
    # Calculate mean losses across all folds for each iteration
    mean_train_loss = [loss / k_folds for loss in accumulated_train_loss]
    mean_test_loss = [loss / k_folds for loss in accumulated_test_loss]

    # Plotting mean losses against iterations
    plt.plot(mean_train_loss, label='Mean Train Loss')
    plt.plot(mean_test_loss, label='Mean Test Loss')
    plt.ylabel('Mean Loss')
    plt.xlabel('Iteration')
    plt.title('Mean Training and Test Loss Across All Folds')
    plt.legend()
    plt.show()

    #Calculates and prints the mean accuracy 
    mean_accuracy_balanced = np.mean(balanced_acc_list)
    print("The balanced mean accuracy is", mean_accuracy_balanced*100)
    
    mean_accuracy_unbalanced = np.mean(acc_list)
    print("The unbalanced mean accuracy is", mean_accuracy_unbalanced*100)

    cm = confusion_matrix(all_y_test_labels, all_y_pred_labels)

    # #Plots train loss against iterations 
    # plt.ylabel("Train loss")
    # plt.xlabel("Iterations ")
    # plt.legend(list(range(1,11)),title="K Fold")
    # plt.show()

    #Visualization of Confusion Matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d")
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    plt.show()        

    # #Plots accuracy in each k fold 
    # plt.ylabel("Accuracy")
    # plt.xlabel("K Fold ")
    # plt.bar(list(range(1,11)),acc_list,color = "#009999") 
    # plt.show()
    # print(test_loss.item())
    return weight_decay, test_loss.item()



# X,y = pre_processing(dataframe)
# Neural_network(X, y, learning_rate=1e-5, weight_decay=0.55, iterations=2000, balanced=False)
# Neural_network(X,y,1e-4,0.05,100,balanced=True)

def plot_weight_decay(list_of_weight_decays, balanced):
    X,y = pre_processing(dataframe)
    iterations = 100
    learning_rate = 1e-5

    plot_x_list = []
    plot_y_list = []
    for weight_decay in list_of_weight_decays:
        plot_x, plot_y = Neural_network(X,y,learning_rate,weight_decay,iterations,balanced)
        plot_x_list.append(plot_x)
        plot_y_list.append(plot_y)
    plt.scatter(plot_x_list,plot_y_list)
    plt.show()      


plot_weight_decay([0.025, 0.05, 0.075, 0.1, 1], False)
