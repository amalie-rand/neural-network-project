import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.model_selection import KFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score


# Data preprocessing
def pre_processing(df):
    missing = {}
    for idx, boolean in enumerate(df.isna().any()):
        if boolean == True:
            missing[df.columns[idx]] = sum(df.iloc[:,idx].isna())

    # Replacing nan values
    df[list(missing.keys())] = df[list(missing.keys())].fillna(df[list(missing.keys())].median())  

    # Deleting non-relevant columns
    columns_to_delete = ['main.disorder','eeg.date','no.',"Unnamed: 122"]
    df = df.drop(columns_to_delete, axis=1)

    # Standardize all columns except our encoded categorised columns
    columns_standardize = df.columns.difference(['specific.disorder', 'sex'])
    scaler = StandardScaler()
    df[columns_standardize] = scaler.fit_transform(df[columns_standardize])

    # Encoding 'sex' as 0 for M and 1 for F
    df['sex'] = df['sex'].map({'M': 0, 'F': 1})

    #Encoding columns and defining X and y
    encoder = OneHotEncoder(sparse_output=False)
    y = encoder.fit_transform(df[['specific.disorder']])  

    X = df.loc[:, df.columns != "specific.disorder"]
    return X, y, encoder


def Neural_network(X, y, encoder, learning_rate, weight_decay, iterations, weighted_classes):
    device = torch.device('cpu')

    # Manually set random seed for reproducability
    torch.manual_seed(42)

    # Number of K folds and iterations  
    k_folds = 10
    T = iterations

    # Empty lists for future use for confusion matrix
    all_y_test_labels = []
    all_y_pred_labels = []

    balanced_acc_list = []
    acc_list = []
    
    # Lists to store the mean loss 
    accumulated_train_loss = []
    accumulated_test_loss = []

    for train, test in KFold(n_splits=k_folds, shuffle=True, random_state=69).split(X,y):
        # Divides data set into a test and train set 
        X_train, y_train = X.iloc[train], y[train]
        X_test, y_test = X.iloc[test], y[test]

        # Defining our test and train set as tensors   
        X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
        
        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

        # Our neural network model
        model = torch.nn.Sequential(
            torch.nn.Linear(1144, 300),
            torch.nn.ReLU(),
            torch.nn.Linear(300, 12)
        )
        model.to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)


        # Defining loss function based on if the classes are weighted or not
        if weighted_classes:
            # Balancing the L2 regularization 
            y_train_1d = np.argmax(y_train, axis=1)
            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_1d), y=y_train_1d)
            class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)

            loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)

        elif not weighted_classes:
            loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')

        train_loss_list = []
        test_loss_list = []
        
        for t in tqdm(range(T)):
            # Forward pass: compute predicted y by passing x to the model.
            y_train_pred = model(X_train_tensor)

            # Compute and save loss.
            train_loss = loss_fn(y_train_pred, y_train_tensor)
            optimizer.zero_grad()

            # Backward pass: compute gradient of the loss with respect to model
            train_loss.backward()

            # Calling the step function on an Optimizer makes an update to its parameters
            optimizer.step()   

            # Adds our training loss values to a list 
            train_loss_list.append(train_loss.item())

            # Compute test loss
            with torch.no_grad():
                y_test_pred = model(X_test_tensor)
                test_loss = loss_fn(y_test_pred, y_test_tensor)
                test_loss_list.append(test_loss.item())
            
            if t < len(accumulated_train_loss):
                accumulated_train_loss[t] += train_loss.item()
                accumulated_test_loss[t] += test_loss.item()
            else:
                accumulated_train_loss.append(train_loss.item())
                accumulated_test_loss.append(test_loss.item())

        if weighted_classes:
            y_test_1d = np.argmax(y_test, axis=1)
            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_test_1d), y=y_test_1d)
            class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)

        y_test_pred = model(X_test_tensor)

        # Test loss balanced and unbalanced
        test_loss = loss_fn(y_test_pred, y_test_tensor)
      
        # Transforming prediction to guess
        highest = y_test_pred.argmax (1)
        guess = torch.zeros(y_test_pred.shape).scatter(1, highest.unsqueeze (1), 1.0)
        # Prints the model's test guess for each K fold 
        guess_dict = {}
        for item in guess:
            guess_dict[str(item)] = guess_dict.get(str(item), 0 ) + 1
        # print(guess_dict)
        
        # Labels used for balanced accuracy 
        y_pred_labels = torch.argmax(y_test_pred, dim=1).cpu().numpy()
        y_test_labels = torch.argmax(y_test_tensor, dim=1).cpu().numpy()
        all_y_test_labels.extend(y_test_labels)
        all_y_pred_labels.extend(y_pred_labels)

        # Balanced accuracy
        balanced_acc = balanced_accuracy_score(y_test_labels, y_pred_labels)
        balanced_acc_list.append(balanced_acc)

        # Overall accuarcy
        acc = (sum(torch.argmax(guess,dim=1) == torch.argmax(y_test_tensor,dim=1))/len(y_test_tensor))
        acc_list.append(acc)

    
    # Calculate mean losses across all folds for each iteration
    mean_train_loss = [loss / k_folds for loss in accumulated_train_loss]
    mean_test_loss = [loss / k_folds for loss in accumulated_test_loss]

    # Plotting mean losses against iterations
    plt.plot(mean_train_loss, label='Mean Train Loss')
    plt.plot(mean_test_loss, label='Mean Test Loss')
    plt.ylabel('Mean Loss')
    plt.xlabel('Iteration')
    plt.title('Mean Training and Test Loss Across All Folds')
    plt.legend()
    plt.show()

    # Calculates and prints the mean accuracy 
    mean_accuracy_balanced = np.mean(balanced_acc_list)
    print("The balanced mean accuracy is", mean_accuracy_balanced*100)
    
    mean_accuracy_overall = np.mean(acc_list)
    print("The overall mean accuracy is", mean_accuracy_overall*100)

    #Visualization of Confusion Matrix
    cm = confusion_matrix(all_y_test_labels, all_y_pred_labels)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d")
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    plt.show()        
 
    class_accuracies = cm.diagonal() / cm.sum(axis=1)
    class_labels = encoder.categories_[0]  # Assuming 'specific.disorder' is the only column encoded
    for label, accuracy in zip(class_labels, class_accuracies):
        print(f"Accuracy for class '{label}': {accuracy * 100:.2f}%")


dataframe = pd.read_csv('./neural-network-project/EEG.machinelearing_data_BRMH.csv')
X, y, encoder = pre_processing(dataframe)
Neural_network(X, y, encoder, learning_rate=1e-2, weight_decay=0.1, iterations=1000, weighted_classes=True)
Neural_network(X, y, encoder, learning_rate=1e-3, weight_decay=0.15, iterations=1000, weighted_classes=False)
