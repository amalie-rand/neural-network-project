### main code

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from sklearn.metrics import confusion_matrix
import seaborn as sns

dataframe = pd.read_csv('./neural-network-project/EEG.machinelearing_data_BRMH.csv')

#Data preprocessing
def pre_processing(df):
    missing = {}
    for idx, boolean in enumerate(df.isna().any()):
        if boolean == True:
            missing[df.columns[idx]] = sum(df.iloc[:,idx].isna())

    #Replacing nan values
    df[list(missing.keys())] = df[list(missing.keys())].fillna(df[list(missing.keys())].median())  

    # Deleting non-relevant columns
    columns_to_delete = ['main.disorder','eeg.date','no.',"Unnamed: 122"]
    df = df.drop(columns_to_delete, axis=1)

    # Standardize all columns except our encoded categorised columns
    columns_standardize = df.columns.difference(['specific.disorder', 'sex'])
    scaler = StandardScaler()
    df[columns_standardize] = scaler.fit_transform(df[columns_standardize])

    # Encoding 'sex' as 0 for M and 1 for F
    df['sex'] = df['sex'].map({'M': 0, 'F': 1})

    #Encoding columns and defining X and y
    encoder = OneHotEncoder(sparse_output=False)
    y = encoder.fit_transform(df[['specific.disorder']])  

    X = df.loc[:, df.columns != "specific.disorder"]
    return X,y

X,y = pre_processing(dataframe)

device = torch.device('cpu')

#Manually set random seed for reproducability
torch.manual_seed(42)

#K folds and number of iterations 
k_folds = 10
T = 2000

#Empty lists for future use for confusion matrix
all_y_test_labels = []
all_y_pred_labels = []

################################################################################################################################################################################
#lists to store the mean loss 
accumulated_train_loss = []
accumulated_test_loss = []
################################################################################################################################################################################

#list for accuracy
acc_list = []

for train, test in KFold(n_splits=k_folds, shuffle=True, random_state=69).split(X,y):
    model = torch.nn.Sequential(
        torch.nn.Linear(1144, 1000),
        torch.nn.ReLU(),
        torch.nn.Linear(1000, 500),
        torch.nn.ReLU(),
        torch.nn.Linear(500, 250),
        torch.nn.ReLU(),
        torch.nn.Linear(250, 12),
    )

    model.to(device)
    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')
    
    learning_rate = 1e-3
    weight_decay = 0.1
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    #Divides data set into a test and train set 
    X_train, y_train = X.iloc[train], y[train]
    X_test, y_test = X.iloc[test], y[test]
    
    #Defining our test and train set as tensors  
    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
    
    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

    train_loss_list = []
    test_loss_list = []

    for t in tqdm(range(T)):
        # Forward pass: compute predicted y by passing x to the model.
        y_train_pred = model(X_train_tensor)

        # Compute and save loss.
        train_loss = loss_fn(y_train_pred, y_train_tensor)
        optimizer.zero_grad()

        # Backward pass: compute gradient of the loss with respect to model
        train_loss.backward()

        # Calling the step function on an Optimizer makes an update to its parameters
        optimizer.step()   

        # Adds our training loss values to a list 
        train_loss_list.append(train_loss.item())

#################################################################################################################################################################################
        # Compute test loss
        with torch.no_grad():
            y_test_pred = model(X_test_tensor)
            test_loss = loss_fn(y_test_pred, y_test_tensor)
            test_loss_list.append(test_loss.item())
            
        if t < len(accumulated_train_loss):
            accumulated_train_loss[t] += train_loss.item()
            accumulated_test_loss[t] += test_loss.item()
        else:
            accumulated_train_loss.append(train_loss.item())
            accumulated_test_loss.append(test_loss.item())
#################################################################################################################################################################################
    
    y_test_pred = model(X_test_tensor)

    #transforming prediction to guess
    highest = y_test_pred.argmax(1)
    guess = torch.zeros(y_test_pred.shape).scatter(1, highest.unsqueeze (1), 1.0)

    test_loss = loss_fn(y_test_pred, y_test_tensor)   

    #Calculating the accuracy of our prediction 
    acc = (sum(torch.argmax(guess,dim=1) == torch.argmax(y_test_tensor,dim=1))/len(y_test_tensor))*100
    print(acc)
    acc_list.append(acc)

    #Variables for confusion matrix 
    y_pred_labels = torch.argmax(y_test_pred, dim=1).cpu().numpy()
    y_test_labels = torch.argmax(y_test_tensor, dim=1).cpu().numpy()
    all_y_test_labels.extend(y_test_labels)
    all_y_pred_labels.extend(y_pred_labels)

    #Prints the model's test guess for each K fold 
    guess_dict = {}
    for item in guess:
        guess_dict[str(item)] = guess_dict.get(str(item), 0 ) + 1
    print(guess_dict)

#################################################################################################################################################################################
# Calculate mean losses across all folds for each iteration
mean_train_loss = [loss / k_folds for loss in accumulated_train_loss]
mean_test_loss = [loss / k_folds for loss in accumulated_test_loss]

# Plotting mean losses against iterations
plt.plot(mean_train_loss, label='Mean Train Loss')
plt.plot(mean_test_loss, label='Mean Test Loss')
plt.ylabel('Mean Loss')
plt.xlabel('Iteration')
plt.title('Mean Training and Test Loss Across All Folds')
plt.legend()
plt.show()
#################################################################################################################################################################################

#Calculates and prints the mean accuracy 
mean_accuracy = np.mean(acc_list)
print("The mean accuracy is", mean_accuracy)


#Calculates confusion matrix 
cm = confusion_matrix(all_y_test_labels, all_y_pred_labels)

#Plots confusion matrix 
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d")
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()  

#Plots accuracy in each k fold 
plt.ylabel("Accuracy")
plt.xlabel("K Fold ")
plt.bar(list(range(1,11)),acc_list,color = "#009999") 
plt.show()
